{% extends "base.html" %} 
{% block title %}Audio Transcription App{% endblock %} 

{% block content %}
<div class="container mx-auto px-4 py-8 max-w-4xl">
  <h1 class="text-3xl font-bold text-center mb-8">Audio Transcription App</h1>
  
  <!-- Configuration Section -->
  <div class="bg-white rounded-lg shadow-md p-6 mb-6">
    <h2 class="text-xl font-semibold mb-4">Configuration</h2>
    <div class="space-y-4">
      <div>
        <label for="openai-api-key" class="block text-sm font-medium text-gray-700 mb-1">
          OpenAI API Key
        </label>
        <input
          type="password"
          id="openai-api-key"
          placeholder="Enter your OpenAI API key"
          class="w-full px-3 py-2 border border-gray-300 rounded-md focus:outline-none focus:ring-2 focus:ring-blue-500"
          required
        />
      </div>
      <div>
        <label for="language" class="block text-sm font-medium text-gray-700 mb-1">
          Language (optional)
        </label>
        <select
          id="language"
          class="w-full px-3 py-2 border border-gray-300 rounded-md focus:outline-none focus:ring-2 focus:ring-blue-500"
        >
          <option value="">Auto-detect</option>
          <option value="en">English</option>
          <option value="zh">Chinese</option>
          <option value="ja">Japanese</option>
          <option value="ko">Korean</option>
          <option value="es">Spanish</option>
          <option value="fr">French</option>
          <option value="de">German</option>
        </select>
      </div>
    </div>
  </div>

  <!-- Recording Controls -->
  <div class="bg-white rounded-lg shadow-md p-6 mb-6">
    <h2 class="text-xl font-semibold mb-4">Real-time Recording</h2>
    <div class="flex flex-col items-center space-y-4">
      <div class="flex items-center space-x-4">
        <button
          id="record-btn"
          class="bg-red-500 hover:bg-red-600 text-white px-6 py-3 rounded-full font-semibold transition-colors"
        >
          üé§ Start Real-time Recording
        </button>
        <button
          id="stop-btn"
          class="bg-gray-500 hover:bg-gray-600 text-white px-6 py-3 rounded-full font-semibold transition-colors disabled:opacity-50"
          disabled
        >
          ‚èπÔ∏è Stop Recording
        </button>
      </div>
      
      <div id="recording-status" class="text-center">
        <div id="recording-indicator" class="hidden">
          <div class="inline-block w-3 h-3 bg-red-500 rounded-full animate-pulse mr-2"></div>
          <span class="text-red-600 font-medium">Recording & Transcribing...</span>
        </div>
      </div>
      
      <div id="audio-visualizer" class="w-full max-w-md h-16 bg-gray-100 rounded-lg flex items-center justify-center hidden">
        <canvas id="visualizer-canvas" width="400" height="60"></canvas>
      </div>
      
      <div class="text-sm text-gray-600 text-center">
        <p>Recording in 5-second chunks for real-time transcription</p>
      </div>
    </div>
  </div>

  <!-- Transcription Results -->
  <div class="bg-white rounded-lg shadow-md p-6">
    <h2 class="text-xl font-semibold mb-4">Transcription Results</h2>
    <div id="transcription-results" class="space-y-4">
      <div class="text-gray-500 text-center py-8">
        Start recording to see transcription results here...
      </div>
    </div>
  </div>

  <!-- Error Messages -->
  <div id="error-message" class="hidden bg-red-100 border border-red-400 text-red-700 px-4 py-3 rounded mt-4">
    <span id="error-text"></span>
  </div>
</div>

<style>
  @keyframes fadeIn {
    from { opacity: 0; transform: translateY(10px); }
    to { opacity: 1; transform: translateY(0); }
  }
  
  .animate-fade-in {
    animation: fadeIn 0.3s ease-out;
  }
</style>
{% endblock %}

{% block scripts %}
<script>
class AudioTranscriber {
  constructor() {
    this.mediaRecorder = null;
    this.audioChunks = [];
    this.isRecording = false;
    this.audioContext = null;
    this.analyser = null;
    this.visualizerCanvas = null;
    this.visualizerCtx = null;
    this.animationId = null;
    this.currentChunk = [];
    this.isTranscribing = false;
    
    this.initializeElements();
    this.setupEventListeners();
  }

  initializeElements() {
    this.recordBtn = document.getElementById('record-btn');
    this.stopBtn = document.getElementById('stop-btn');
    this.recordingStatus = document.getElementById('recording-status');
    this.recordingIndicator = document.getElementById('recording-indicator');
    this.audioVisualizer = document.getElementById('audio-visualizer');
    this.visualizerCanvas = document.getElementById('visualizer-canvas');
    this.visualizerCtx = this.visualizerCanvas.getContext('2d');
    this.transcriptionResults = document.getElementById('transcription-results');
    this.errorMessage = document.getElementById('error-message');
    this.errorText = document.getElementById('error-text');
    this.apiKeyInput = document.getElementById('openai-api-key');
    this.languageSelect = document.getElementById('language');
  }

  setupEventListeners() {
    this.recordBtn.addEventListener('click', () => this.startRecording());
    this.stopBtn.addEventListener('click', () => this.stopRecording());
  }

  async startRecording() {
    try {
      const apiKey = this.apiKeyInput.value.trim();
      if (!apiKey) {
        this.showError('Please enter your OpenAI API key');
        return;
      }

      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true
        } 
      });

      this.setupMediaRecorder(stream);
      this.isRecording = true;
      this.updateUI();
      this.setupAudioVisualizer(stream);
      
    } catch (error) {
      console.error('Error starting recording:', error);
      this.showError('Error accessing microphone. Please check permissions.');
    }
  }

  setupMediaRecorder(stream) {
    this.mediaRecorder = new MediaRecorder(stream, {
      mimeType: 'audio/webm;codecs=opus'
    });

    this.currentChunk = [];
    
    this.mediaRecorder.ondataavailable = (event) => {
      if (event.data.size > 0) {
        this.currentChunk.push(event.data);
        // Process the chunk immediately when data is available
        this.processCurrentChunk();
      }
    };

    this.mediaRecorder.onstop = () => {
      // When MediaRecorder stops, process any remaining data
      if (this.currentChunk.length > 0) {
        this.processCurrentChunk();
      }
      // Restart recording if we're still supposed to be recording
      if (this.isRecording) {
        this.setupMediaRecorder(stream);
        this.mediaRecorder.start(5000);
      }
    };

    // Start recording with 5-second chunks
    this.mediaRecorder.start(5000);
  }

  stopRecording() {
    if (this.mediaRecorder && this.isRecording) {
      this.isRecording = false;
      this.mediaRecorder.stop();
      this.updateUI();
      this.stopAudioVisualizer();
      
      // Process any remaining audio in the current chunk
      if (this.currentChunk.length > 0) {
        this.processCurrentChunk();
      }
    }
  }

  updateUI() {
    if (this.isRecording) {
      this.recordBtn.disabled = true;
      this.stopBtn.disabled = false;
      this.recordingIndicator.classList.remove('hidden');
      this.audioVisualizer.classList.remove('hidden');
    } else {
      this.recordBtn.disabled = false;
      this.stopBtn.disabled = true;
      this.recordingIndicator.classList.add('hidden');
      this.audioVisualizer.classList.add('hidden');
    }
  }

  setupAudioVisualizer(stream) {
    this.audioContext = new (window.AudioContext || window.webkitAudioContext)();
    this.analyser = this.audioContext.createAnalyser();
    const source = this.audioContext.createMediaStreamSource(stream);
    
    source.connect(this.analyser);
    this.analyser.fftSize = 256;
    
    this.drawVisualizer();
  }

  drawVisualizer() {
    if (!this.isRecording) return;
    
    const bufferLength = this.analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    
    this.analyser.getByteFrequencyData(dataArray);
    
    this.visualizerCtx.fillStyle = '#f3f4f6';
    this.visualizerCtx.fillRect(0, 0, this.visualizerCanvas.width, this.visualizerCanvas.height);
    
    const barWidth = (this.visualizerCanvas.width / bufferLength) * 2.5;
    let barHeight;
    let x = 0;
    
    for (let i = 0; i < bufferLength; i++) {
      barHeight = (dataArray[i] / 255) * this.visualizerCanvas.height;
      
      this.visualizerCtx.fillStyle = `rgb(${barHeight + 100}, 50, 50)`;
      this.visualizerCtx.fillRect(x, this.visualizerCanvas.height - barHeight, barWidth, barHeight);
      
      x += barWidth + 1;
    }
    
    this.animationId = requestAnimationFrame(() => this.drawVisualizer());
  }

  stopAudioVisualizer() {
    if (this.animationId) {
      cancelAnimationFrame(this.animationId);
    }
    if (this.audioContext) {
      this.audioContext.close();
    }
  }

  async processCurrentChunk() {
    if (this.currentChunk.length === 0 || this.isTranscribing) {
      return;
    }

    // Create a copy of the current chunk and reset it
    const chunkToProcess = [...this.currentChunk];
    this.currentChunk = [];

    if (chunkToProcess.length === 0) {
      return;
    }

    const audioBlob = new Blob(chunkToProcess, { type: 'audio/webm;codecs=opus' });
    await this.transcribeAudio(audioBlob);
  }

  async processRecording() {
    if (this.audioChunks.length === 0) {
      this.showError('No audio data recorded');
      return;
    }

    const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm;codecs=opus' });
    await this.transcribeAudio(audioBlob);
  }

  async transcribeAudio(audioBlob) {
    try {
      this.isTranscribing = true;
      
      // Convert audio to the format expected by OpenAI
      const formData = new FormData();
      formData.append('file', audioBlob, 'recording.webm');
      formData.append('model', 'gpt-4o-mini-transcribe');
      
      const language = this.languageSelect.value;
      if (language) {
        formData.append('language', language);
      }

      const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKeyInput.value.trim()}`
        },
        body: formData
      });

      if (!response.ok) {
        const errorData = await response.json();
        throw new Error(errorData.error?.message || `HTTP error! status: ${response.status}`);
      }

      const result = await response.json();
      
      // Only display if there's actual text (not just silence)
      if (result.text && result.text.trim().length > 0) {
        this.displayTranscription(result.text);
      }
      
    } catch (error) {
      console.error('Transcription error:', error);
      this.showError(`Transcription failed: ${error.message}`);
    } finally {
      this.isTranscribing = false;
    }
  }

  showLoading() {
    // Don't show loading for real-time transcription to avoid UI flicker
    // The recording indicator already shows that transcription is happening
  }

  displayTranscription(text) {
    const timestamp = new Date().toLocaleTimeString();
    const transcriptionItem = document.createElement('div');
    transcriptionItem.className = 'bg-blue-50 rounded-lg p-4 border-l-4 border-blue-500 mb-2 animate-fade-in';
    transcriptionItem.innerHTML = `
      <div class="flex justify-between items-start mb-2">
        <span class="text-sm text-gray-500">${timestamp}</span>
        <button class="text-gray-400 hover:text-gray-600" onclick="this.parentElement.parentElement.remove()">
          ‚úï
        </button>
      </div>
      <p class="text-gray-800">${text}</p>
    `;
    
    // If this is the first transcription, clear the placeholder
    if (this.transcriptionResults.querySelector('.text-gray-500.text-center')) {
      this.transcriptionResults.innerHTML = '';
    }
    
    this.transcriptionResults.insertBefore(transcriptionItem, this.transcriptionResults.firstChild);
    
    // Auto-scroll to show the latest transcription
    transcriptionItem.scrollIntoView({ behavior: 'smooth', block: 'nearest' });
  }

  showError(message) {
    this.errorText.textContent = message;
    this.errorMessage.classList.remove('hidden');
    setTimeout(() => {
      this.errorMessage.classList.add('hidden');
    }, 5000);
  }
}

// Initialize the app when the page loads
document.addEventListener('DOMContentLoaded', () => {
  new AudioTranscriber();
});
</script>
{% endblock %}
